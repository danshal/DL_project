{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wav2vec_SV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danshal/DL_project/blob/develop/wav2vec_SV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wl_onLCsnYA"
      },
      "source": [
        "##Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wZdYezr-O3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99960e4c-a929-4525-e505-a0290a2ad8fe"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfi2rXAIs9rR"
      },
      "source": [
        "##Install fairseq and more packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMEDrdPctBfp",
        "outputId": "13fd8769-d7bb-4518-bdc7-0ebe8d9a6ab7"
      },
      "source": [
        "!pip install soundfile git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\r\n",
        "!pip install fairseq\r\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\r\n",
        "#!mkdir fairseq_code\r\n",
        "#%cd /content/drive/MyDrive/DeepProject/wav2vec/fairseq_code/\r\n",
        "#!git clone https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\n",
            "  Cloning git://github.com/pytorch/fairseq.git (to revision b8ea8a9b72c82192da07e3377adf4ebbde16716d) to /tmp/pip-req-build-zr07xw7p\n",
            "  Running command git clone -q git://github.com/pytorch/fairseq.git /tmp/pip-req-build-zr07xw7p\n",
            "  Running command git checkout -q b8ea8a9b72c82192da07e3377adf4ebbde16716d\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.14.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (0.29.21)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (2019.12.20)\n",
            "Collecting hydra-core<1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+b8ea8a9) (3.7.4.3)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 12.1MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/14/47/43fdee732cda080367c7d2a1ee5aee3b31e49baf629029a386df458d2dd3/portalocker-2.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==1.0.0a0+b8ea8a9) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==1.0.0a0+b8ea8a9) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (5.1.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 14.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (3.4.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+b8ea8a9-cp36-cp36m-linux_x86_64.whl size=2713508 sha256=80d9b6462d2a66ba876e85cf02f92a3efddfa71d722436aa77d229aad0943f31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_49o3lkx/wheels/39/26/93/08a9f837c7e94436027d116b1ffe57822725b2caf742649fb3\n",
            "Successfully built fairseq\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141231 sha256=fcfec07ac6571b4c8e52b681ce22614dad1a23129c491aacbff85b6b57f7bee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, portalocker, sacrebleu, antlr4-python3-runtime, hydra-core, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq-1.0.0a0+b8ea8a9 hydra-core-1.0.6 omegaconf-2.0.6 portalocker-2.2.0 sacrebleu-1.5.0\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (1.0.0a0+b8ea8a9)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
            "Requirement already satisfied: hydra-core<1.1 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.0.6)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq) (5.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq) (5.4.1)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq) (3.4.0)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 249kB/s \n",
            "\u001b[?25hCollecting torchaudio===0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/f9/618434cf4e46dc975871e1516f5499abef6564ab4366f9b2321ee536be14/torchaudio-0.7.2-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchaudio-0.7.2 torchvision-0.8.2+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aagqz11s7f1"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk5-AwDdtm9o"
      },
      "source": [
        "#General imports\r\n",
        "import argparse\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import os.path as osp\r\n",
        "import pprint\r\n",
        "import soundfile as sf\r\n",
        "from typing import Tuple\r\n",
        "#facebook team framework\r\n",
        "import fairseq\r\n",
        "#pytorch\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torchaudio\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchaudio.datasets.utils import (\r\n",
        "    download_url,\r\n",
        "    extract_archive,\r\n",
        "    walk_files,\r\n",
        ")\r\n",
        "\r\n",
        "#matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#numpy\r\n",
        "import numpy as np\r\n",
        "#pandas\r\n",
        "import pandas as pd\r\n",
        "try:\r\n",
        "    import tqdm\r\n",
        "except:\r\n",
        "    print(\"Install tqdm to use --log-format=tqdm\")\r\n",
        "\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPN7qCOwll0k",
        "outputId": "ae05756f-6bed-4afb-cd7e-7a200e8938f9"
      },
      "source": [
        "import torch\r\n",
        "print(torch.__version__)\r\n",
        "!nvcc --version\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)\r\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.1+cu101\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVmPE6q7t_Eq"
      },
      "source": [
        "##Get Trained Model & Try Sanity Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nedhgxmcuDaS",
        "outputId": "8358021c-a7cb-4f8b-d31c-d3a887203636"
      },
      "source": [
        "cp_path = cp_path = '/content/drive/My Drive/DeepProject/wav2vec/wav2vec_large.pt'\r\n",
        "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n",
        "model = model[0]\r\n",
        "model.eval()\r\n",
        "\r\n",
        "wav_input_16khz = torch.randn(1,10000)\r\n",
        "z = model.feature_extractor(wav_input_16khz)\r\n",
        "c = model.feature_aggregator(z)\r\n",
        "print(f'input shape = {wav_input_16khz.shape} ; z shape = {z.shape} ; z shape = {c.shape}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape = torch.Size([1, 10000]) ; z shape = torch.Size([1, 512, 60]) ; z shape = torch.Size([1, 512, 60])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euSpEw7Z3QT0"
      },
      "source": [
        "#@title Hyperparametes\n",
        "batch_size = 32 #@param {type:\"number\"}\n",
        "epochs = 100 #@param {type:\"number\"}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "optimizer_type = \"Adam\" #@param [\"Adam\", \"AdamW\", \"SGD\"]\n",
        "window_sec = 3 #@param {type:\"slider\", min:0, max:15, step:1}\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-sCUryI1ptJ"
      },
      "source": [
        "###Download Dataset\r\n",
        "###Information about LibriSpeech dataset:\r\n",
        "1. Total of 982 hours of 16KHz read English speech\r\n",
        "2. 2484 speakers in this corpus.\r\n",
        "3. Suppose to be reasonably balances in terms of gender and per-speaker duration\r\n",
        "4. Allowed urls for LIBRISPEECH - \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\", \"train-clean-100\", \"train-clean-360\" and \"train-other-500\". (default: \"train-clean-100\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6KWwI91ruB"
      },
      "source": [
        "# rootDir = '/content/drive/MyDrive/DeepProject/wav2vec/'\r\n",
        "# # Download training data\r\n",
        "# train_data = torchaudio.datasets.LIBRISPEECH(f'{rootDir}/dataset/', download=True)\r\n",
        "# # Download test data\r\n",
        "# test_data = torchaudio.datasets.LIBRISPEECH(f'{rootDir}/dataset/', url='test-clean', download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRH31dJMM9OG"
      },
      "source": [
        "##Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9sfMLUeM_0_"
      },
      "source": [
        "def read_audio(fname):\r\n",
        "    ''' Load an audio file and return PCM along with the sample rate '''\r\n",
        "\r\n",
        "    wav, sr = sf.read(fname)\r\n",
        "    assert sr == 16e3\r\n",
        "\r\n",
        "    return wav, 16e3\r\n",
        "\r\n",
        "\r\n",
        "def audio2tensor(fname):\r\n",
        "  ''' This function will convert audio samples to tensor '''\r\n",
        "  input, _ = read_audio(fname)\r\n",
        "  input_tensor = torch.from_numpy(input).float()\r\n",
        "  return input_tensor.reshape(1, input_tensor.size(0))\r\n",
        "\r\n",
        "\r\n",
        "def get_audio_repr(fname):\r\n",
        "  ''' This function will get audio as input and will output its representation\r\n",
        "      as wav2vec model outputs '''\r\n",
        "  input_tensor = audio2tensor(fname)\r\n",
        "  z = model.feature_extractor(input_tensor)\r\n",
        "  c = model.feature_aggregator(z)\r\n",
        "  return c\r\n",
        "\r\n",
        "\r\n",
        "def get_tensor_repr(input_tensor):\r\n",
        "  '''This function will get a tensor and return its representation as wav2vec\r\n",
        "     model outputs'''\r\n",
        "  with torch.no_grad():\r\n",
        "    z = model.feature_extractor(input_tensor)\r\n",
        "    c = model.feature_aggregator(z)\r\n",
        "    return c\r\n",
        "\r\n",
        "\r\n",
        "def get_sv_example_generator(data):\r\n",
        "  '''This function gets data that loaded as LIBRISPEECH dataset and yields its\r\n",
        "     wav2vec tensor representation with the speaker id''' \r\n",
        "  train_data_iter = iter(data)\r\n",
        "  for i in train_data_iter:\r\n",
        "    yield [get_tensor_repr(i[0]), i[3]]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRHqwROyKmRy"
      },
      "source": [
        "##Make My Own Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtQG9p38KNyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94dbd20-ecf1-49b0-9802-d2413fbc26b0"
      },
      "source": [
        "rootDir = '/content/drive/MyDrive/DeepProject/wav2vec/'\r\n",
        "URL = \"train-clean-100\"\r\n",
        "FOLDER_IN_ARCHIVE_THREE_SEC_REPR = \"SV_Librispeech_Latent_Dataset_new\"\r\n",
        "FOLDER_IN_ARCHIVE_THREE_SEC_AUDIO = \"SV_Librispeech_Dataset\"\r\n",
        "FOLDER_IN_ARCHIVE_ORIGINAL_LIBRI = \"LibriSpeech\"\r\n",
        "_CHECKSUMS = {\r\n",
        "    \"http://www.openslr.org/resources/12/dev-clean.tar.gz\":\r\n",
        "    \"76f87d090650617fca0cac8f88b9416e0ebf80350acb97b343a85fa903728ab3\",\r\n",
        "    \"http://www.openslr.org/resources/12/dev-other.tar.gz\":\r\n",
        "    \"12661c48e8c3fe1de2c1caa4c3e135193bfb1811584f11f569dd12645aa84365\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-clean.tar.gz\":\r\n",
        "    \"39fde525e59672dc6d1551919b1478f724438a95aa55f874b576be21967e6c23\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-other.tar.gz\":\r\n",
        "    \"d09c181bba5cf717b3dee7d4d592af11a3ee3a09e08ae025c5506f6ebe961c29\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\":\r\n",
        "    \"d4ddd1d5a6ab303066f14971d768ee43278a5f2a0aa43dc716b0e64ecbbbf6e2\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\":\r\n",
        "    \"146a56496217e96c14334a160df97fffedd6e0a04e66b9c5af0d40be3c792ecf\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-other-500.tar.gz\":\r\n",
        "    \"ddb22f27f96ec163645d53215559df6aa36515f26e01dd70798188350adcb6d2\"\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "size = (100, 100)\r\n",
        "#filter options for tranform are:\r\n",
        "#1. PIL.Image.NEAREST -> Pick one nearest pixel from the input image. Ignore all other input pixels.\r\n",
        "#2. PIL.Image.BILINEAR -> For resize calculate the output pixel value using linear interpolation on all pixels that may contribute to the output value. For other transformations linear interpolation over a 2x2 environment in the input image is used.\r\n",
        "#3. PIL.Image.HAMMING -> Produces a sharper image than BILINEAR, doesn’t have dislocations on local level like with BOX.\r\n",
        "#4. PIL.Image.BICUBIC -> For resize calculate the output pixel value using cubic interpolation on all pixels that may contribute to the output value. For other transformations cubic interpolation over a 4x4 environment in the input image is used.\r\n",
        "#5. PIL.Image.LANCZOS -> Calculate the output pixel value using a high-quality Lanczos filter (a truncated sinc) on all pixels that may contribute to the output value\r\n",
        "#For more details -> https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\r\n",
        "transform = transforms.Resize(size)\r\n",
        "\r\n",
        "\r\n",
        "def load_sv_librispeech_item(fileid: str, path: str, ext: str = '.pt') -> Tuple[Tensor, int]:\r\n",
        "    '''This function return example as wav2vec representation or librispeech audio\r\n",
        "       representation from a given path'''\r\n",
        "    speaker_id, utterance_id = fileid.split(\"-\")\r\n",
        "    file_to_load = fileid + ext\r\n",
        "    file_path = os.path.join(path, speaker_id, file_to_load)\r\n",
        "    if ext == '.flac':\r\n",
        "      input, _ = torchaudio.load(file_path) # Load audio - dont care about sample rate\r\n",
        "    else:\r\n",
        "      input = torch.load(file_path)\r\n",
        "    return (input, int(speaker_id))\r\n",
        "\r\n",
        "\r\n",
        "# create my own collate function to avoid stacking same size examples in a batch\r\n",
        "def my_collate(batch):\r\n",
        "    data = [item[0] for item in batch]  #get 512 X ? tensor in item[0]\r\n",
        "    data = tuple(data)\r\n",
        "    data = torch.cat(data)\r\n",
        "    data = data.detach().clone()\r\n",
        "    target = [item[1] for item in batch]        \r\n",
        "    target = torch.LongTensor(target)\r\n",
        "    return [data, target]\r\n",
        "\r\n",
        "\r\n",
        "class SV_LIBRISPEECH(Dataset):\r\n",
        "    \"\"\"Create a Dataset for SV_LibriSpeech.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (str): Path to the directory where the dataset is found or downloaded.\r\n",
        "        url (str, optional): The URL to download the dataset from,\r\n",
        "            or the type of the dataset to dowload.\r\n",
        "            Allowed type values are ``\"dev-clean\"``, ``\"dev-other\"``, ``\"test-clean\"``,\r\n",
        "            ``\"test-other\"``, ``\"train-clean-100\"``, ``\"train-clean-360\"`` and\r\n",
        "            ``\"train-other-500\"``. (default: ``\"train-clean-100\"``)\r\n",
        "        folder_in_archive (str, optional):\r\n",
        "            The top-level directory of the dataset. (default: ``\"LibriSpeech\"``)\r\n",
        "        download (bool, optional):\r\n",
        "            Whether to download the dataset if it is not found at root path. (default: ``False``).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    _ext_audio = \".flac\"\r\n",
        "    _ext_repr = \".pt\"\r\n",
        "\r\n",
        "    def __init__(self, root: str, url: str = URL,\r\n",
        "                 folder_in_archive: str = FOLDER_IN_ARCHIVE_THREE_SEC_REPR,\r\n",
        "                 download: bool = False, is_SV: bool = True, wav2vec_fine_tuning: bool = False) -> None:\r\n",
        "\r\n",
        "        if url in [\r\n",
        "            \"dev-clean\",\r\n",
        "            \"dev-other\",\r\n",
        "            \"test-clean\",\r\n",
        "            \"test-other\",\r\n",
        "            \"train-clean-100\",\r\n",
        "            \"train-clean-360\",\r\n",
        "            \"train-other-500\",\r\n",
        "        ]:\r\n",
        "            ext_archive = \".tar.gz\"\r\n",
        "            base_url = \"http://www.openslr.org/resources/12/\"\r\n",
        "\r\n",
        "            url = os.path.join(base_url, url + ext_archive)\r\n",
        "\r\n",
        "        basename = os.path.basename(url)\r\n",
        "        archive = os.path.join(root, basename)\r\n",
        "\r\n",
        "        basename = basename.split(\".\")[0]\r\n",
        "        if is_SV == False:\r\n",
        "          folder_in_archive = os.path.join(folder_in_archive, basename)\r\n",
        "\r\n",
        "        self._path = os.path.join(root, folder_in_archive)\r\n",
        "\r\n",
        "        if download:\r\n",
        "            if not os.path.isdir(self._path):\r\n",
        "                if not os.path.isfile(archive):\r\n",
        "                    checksum = _CHECKSUMS.get(url, None)\r\n",
        "                    download_url(url, root, hash_value=checksum)\r\n",
        "                extract_archive(archive)\r\n",
        "        self._ext = self._ext_audio\r\n",
        "        if folder_in_archive == FOLDER_IN_ARCHIVE_THREE_SEC_REPR:\r\n",
        "          self._ext = self._ext_repr\r\n",
        "        walker = walk_files(self._path, suffix=self._ext, prefix=False, remove_suffix=True)\r\n",
        "        self._walker = list(walker)\r\n",
        "        self.ft = wav2vec_fine_tuning\r\n",
        "  \r\n",
        "\r\n",
        "    def __getitem__(self, n: int) -> Tuple[Tensor, int]:\r\n",
        "        \"\"\"Load the n-th sample from the dataset.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            n (int): The index of the sample to be loaded\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple: ``(waveform, speaker_id)``\r\n",
        "        \"\"\"\r\n",
        "        fileid = self._walker[n]  #get file name without .flac suffix        \r\n",
        "        #librispeech_item, speaker_id = load_sv_librispeech_item(fileid, self._path, self._ext_audio)\r\n",
        "        return load_sv_librispeech_item(fileid, self._path, self._ext)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "      return len(self._walker)\r\n",
        "\r\n",
        "\r\n",
        "my_train_data = SV_LIBRISPEECH('/content/drive/MyDrive/DeepProject/wav2vec/',\r\n",
        "                               folder_in_archive = FOLDER_IN_ARCHIVE_THREE_SEC_REPR)\r\n",
        "print(f'Number of examples(utterances): {len(my_train_data)}')\r\n",
        "my_train_loader = torch.utils.data.DataLoader(my_train_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples(utterances): 59295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeUd_FB8rcSV"
      },
      "source": [
        "##Get Number Of Utterances Vectors -> Input Width"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7__bAJarjix",
        "outputId": "ec58888a-432a-4d71-ef25-2dbaed5e88cc"
      },
      "source": [
        "example = next(iter(my_train_data))\r\n",
        "width = example[0].shape[2]\r\n",
        "print(f'Input width = {width}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input width = 298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLH9r9LVgXVf"
      },
      "source": [
        "##Create dictionary that maps between output neuron index to speaker id *(We can write a simple script that will rename each directory with rising index)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qkCgRLE9fvxw",
        "outputId": "323c8293-e7b4-4384-d131-77f443a79411"
      },
      "source": [
        "my_train_data_for_map = SV_LIBRISPEECH('/content/drive/MyDrive/DeepProject/wav2vec/',\r\n",
        "                               folder_in_archive = FOLDER_IN_ARCHIVE_THREE_SEC_AUDIO)\r\n",
        "print(f'Number of examples(utterances): {len(my_train_data_for_map)}')\r\n",
        "my_train_loader = torch.utils.data.DataLoader(my_train_data_for_map, batch_size=batch_size, shuffle=True, collate_fn=my_collate)\r\n",
        "#for 251 speakers its going to take about 3 hours on avarage!!!!!\r\n",
        "import time\r\n",
        "import json\r\n",
        "labels_mapping = {}\r\n",
        "it = iter(my_train_data_for_map)\r\n",
        "curr_id = next(it)[1]\r\n",
        "key_count = 0\r\n",
        "labels_mapping[key_count] = curr_id\r\n",
        "time0 = time.time()\r\n",
        "for i in it:\r\n",
        "  if i[1] != curr_id:\r\n",
        "    print(f'Finished mapping speaker id = {curr_id} by = {(time.time() - time0):.2f}[sec]')\r\n",
        "    curr_id = i[1]\r\n",
        "    key_count = key_count + 1\r\n",
        "    labels_mapping[key_count] = curr_id\r\n",
        "    time0 = time.time()\r\n",
        "\r\n",
        "print(labels_mapping)\r\n",
        "#Write mapping to file\r\n",
        "with open('speakers_mapping.json', 'w') as f:\r\n",
        "  f.write(json.dump(labels_mapping, f, indent = 4))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples(utterances): 79010\n",
            "Finished mapping speaker id = 103 by = 66.42[sec]\n",
            "Finished mapping speaker id = 1034 by = 39.53[sec]\n",
            "Finished mapping speaker id = 1040 by = 41.63[sec]\n",
            "Finished mapping speaker id = 1069 by = 68.31[sec]\n",
            "Finished mapping speaker id = 1081 by = 65.00[sec]\n",
            "Finished mapping speaker id = 1088 by = 68.41[sec]\n",
            "Finished mapping speaker id = 1098 by = 49.05[sec]\n",
            "Finished mapping speaker id = 1116 by = 66.94[sec]\n",
            "Finished mapping speaker id = 118 by = 63.79[sec]\n",
            "Finished mapping speaker id = 1183 by = 24.62[sec]\n",
            "Finished mapping speaker id = 1235 by = 68.01[sec]\n",
            "Finished mapping speaker id = 1246 by = 66.32[sec]\n",
            "Finished mapping speaker id = 125 by = 62.16[sec]\n",
            "Finished mapping speaker id = 1263 by = 74.99[sec]\n",
            "Finished mapping speaker id = 1334 by = 69.15[sec]\n",
            "Finished mapping speaker id = 1355 by = 66.46[sec]\n",
            "Finished mapping speaker id = 1363 by = 62.12[sec]\n",
            "Finished mapping speaker id = 1447 by = 68.85[sec]\n",
            "Finished mapping speaker id = 1455 by = 66.45[sec]\n",
            "Finished mapping speaker id = 150 by = 70.80[sec]\n",
            "Finished mapping speaker id = 1502 by = 61.81[sec]\n",
            "Finished mapping speaker id = 1553 by = 64.17[sec]\n",
            "Finished mapping speaker id = 1578 by = 71.43[sec]\n",
            "Finished mapping speaker id = 1594 by = 68.32[sec]\n",
            "Finished mapping speaker id = 1624 by = 58.91[sec]\n",
            "Finished mapping speaker id = 163 by = 69.67[sec]\n",
            "Finished mapping speaker id = 1723 by = 71.98[sec]\n",
            "Finished mapping speaker id = 1737 by = 69.98[sec]\n",
            "Finished mapping speaker id = 1743 by = 59.49[sec]\n",
            "Finished mapping speaker id = 1841 by = 69.67[sec]\n",
            "Finished mapping speaker id = 1867 by = 64.30[sec]\n",
            "Finished mapping speaker id = 1898 by = 68.67[sec]\n",
            "Finished mapping speaker id = 19 by = 68.74[sec]\n",
            "Finished mapping speaker id = 1926 by = 63.03[sec]\n",
            "Finished mapping speaker id = 196 by = 71.28[sec]\n",
            "Finished mapping speaker id = 1963 by = 65.32[sec]\n",
            "Finished mapping speaker id = 1970 by = 64.75[sec]\n",
            "Finished mapping speaker id = 198 by = 63.91[sec]\n",
            "Finished mapping speaker id = 1992 by = 34.67[sec]\n",
            "Finished mapping speaker id = 200 by = 68.57[sec]\n",
            "Finished mapping speaker id = 2002 by = 66.82[sec]\n",
            "Finished mapping speaker id = 2007 by = 67.94[sec]\n",
            "Finished mapping speaker id = 201 by = 66.01[sec]\n",
            "Finished mapping speaker id = 2092 by = 71.06[sec]\n",
            "Finished mapping speaker id = 211 by = 56.71[sec]\n",
            "Finished mapping speaker id = 2136 by = 67.01[sec]\n",
            "Finished mapping speaker id = 2159 by = 72.50[sec]\n",
            "Finished mapping speaker id = 2182 by = 69.95[sec]\n",
            "Finished mapping speaker id = 2196 by = 70.16[sec]\n",
            "Finished mapping speaker id = 226 by = 72.40[sec]\n",
            "Finished mapping speaker id = 2289 by = 66.02[sec]\n",
            "Finished mapping speaker id = 229 by = 52.86[sec]\n",
            "Finished mapping speaker id = 233 by = 69.75[sec]\n",
            "Finished mapping speaker id = 2384 by = 58.53[sec]\n",
            "Finished mapping speaker id = 2391 by = 65.88[sec]\n",
            "Finished mapping speaker id = 2416 by = 69.08[sec]\n",
            "Finished mapping speaker id = 2436 by = 68.28[sec]\n",
            "Finished mapping speaker id = 248 by = 71.38[sec]\n",
            "Finished mapping speaker id = 250 by = 67.10[sec]\n",
            "Finished mapping speaker id = 2514 by = 70.69[sec]\n",
            "Finished mapping speaker id = 2518 by = 65.68[sec]\n",
            "Finished mapping speaker id = 254 by = 65.42[sec]\n",
            "Finished mapping speaker id = 26 by = 66.57[sec]\n",
            "Finished mapping speaker id = 2691 by = 66.85[sec]\n",
            "Finished mapping speaker id = 27 by = 43.01[sec]\n",
            "Finished mapping speaker id = 2764 by = 66.71[sec]\n",
            "Finished mapping speaker id = 2817 by = 126.94[sec]\n",
            "Finished mapping speaker id = 2836 by = 67.12[sec]\n",
            "Finished mapping speaker id = 2843 by = 65.63[sec]\n",
            "Finished mapping speaker id = 289 by = 53.92[sec]\n",
            "Finished mapping speaker id = 2893 by = 65.46[sec]\n",
            "Finished mapping speaker id = 2910 by = 59.23[sec]\n",
            "Finished mapping speaker id = 2911 by = 64.97[sec]\n",
            "Finished mapping speaker id = 2952 by = 64.02[sec]\n",
            "Finished mapping speaker id = 298 by = 66.60[sec]\n",
            "Finished mapping speaker id = 2989 by = 60.03[sec]\n",
            "Finished mapping speaker id = 302 by = 65.10[sec]\n",
            "Finished mapping speaker id = 307 by = 63.39[sec]\n",
            "Finished mapping speaker id = 311 by = 63.33[sec]\n",
            "Finished mapping speaker id = 3112 by = 70.48[sec]\n",
            "Finished mapping speaker id = 3168 by = 64.68[sec]\n",
            "Finished mapping speaker id = 32 by = 126.89[sec]\n",
            "Finished mapping speaker id = 3214 by = 65.93[sec]\n",
            "Finished mapping speaker id = 322 by = 63.57[sec]\n",
            "Finished mapping speaker id = 3235 by = 65.23[sec]\n",
            "Finished mapping speaker id = 3240 by = 62.27[sec]\n",
            "Finished mapping speaker id = 3242 by = 63.49[sec]\n",
            "Finished mapping speaker id = 3259 by = 62.39[sec]\n",
            "Finished mapping speaker id = 328 by = 46.54[sec]\n",
            "Finished mapping speaker id = 332 by = 47.67[sec]\n",
            "Finished mapping speaker id = 3374 by = 63.05[sec]\n",
            "Finished mapping speaker id = 3436 by = 62.69[sec]\n",
            "Finished mapping speaker id = 3440 by = 65.92[sec]\n",
            "Finished mapping speaker id = 3486 by = 63.82[sec]\n",
            "Finished mapping speaker id = 3526 by = 63.93[sec]\n",
            "Finished mapping speaker id = 3607 by = 55.88[sec]\n",
            "Finished mapping speaker id = 3664 by = 67.12[sec]\n",
            "Finished mapping speaker id = 3699 by = 62.57[sec]\n",
            "Finished mapping speaker id = 3723 by = 64.12[sec]\n",
            "Finished mapping speaker id = 374 by = 64.75[sec]\n",
            "Finished mapping speaker id = 3807 by = 59.06[sec]\n",
            "Finished mapping speaker id = 3830 by = 59.61[sec]\n",
            "Finished mapping speaker id = 3857 by = 67.21[sec]\n",
            "Finished mapping speaker id = 3879 by = 65.73[sec]\n",
            "Finished mapping speaker id = 39 by = 66.59[sec]\n",
            "Finished mapping speaker id = 3947 by = 60.91[sec]\n",
            "Finished mapping speaker id = 3982 by = 68.41[sec]\n",
            "Finished mapping speaker id = 3983 by = 68.08[sec]\n",
            "Finished mapping speaker id = 40 by = 68.32[sec]\n",
            "Finished mapping speaker id = 4014 by = 55.47[sec]\n",
            "Finished mapping speaker id = 4018 by = 70.56[sec]\n",
            "Finished mapping speaker id = 403 by = 65.57[sec]\n",
            "Finished mapping speaker id = 405 by = 67.01[sec]\n",
            "Finished mapping speaker id = 4051 by = 62.21[sec]\n",
            "Finished mapping speaker id = 4088 by = 66.96[sec]\n",
            "Finished mapping speaker id = 412 by = 60.72[sec]\n",
            "Finished mapping speaker id = 4137 by = 67.26[sec]\n",
            "Finished mapping speaker id = 4160 by = 67.52[sec]\n",
            "Finished mapping speaker id = 4195 by = 59.53[sec]\n",
            "Finished mapping speaker id = 4214 by = 45.30[sec]\n",
            "Finished mapping speaker id = 426 by = 65.08[sec]\n",
            "Finished mapping speaker id = 4267 by = 66.90[sec]\n",
            "Finished mapping speaker id = 4297 by = 67.30[sec]\n",
            "Finished mapping speaker id = 4340 by = 59.31[sec]\n",
            "Finished mapping speaker id = 4362 by = 71.51[sec]\n",
            "Finished mapping speaker id = 4397 by = 67.73[sec]\n",
            "Finished mapping speaker id = 4406 by = 63.23[sec]\n",
            "Finished mapping speaker id = 441 by = 65.57[sec]\n",
            "Finished mapping speaker id = 4441 by = 62.60[sec]\n",
            "Finished mapping speaker id = 445 by = 26.75[sec]\n",
            "Finished mapping speaker id = 446 by = 65.87[sec]\n",
            "Finished mapping speaker id = 4481 by = 73.53[sec]\n",
            "Finished mapping speaker id = 458 by = 59.50[sec]\n",
            "Finished mapping speaker id = 460 by = 73.48[sec]\n",
            "Finished mapping speaker id = 4640 by = 63.41[sec]\n",
            "Finished mapping speaker id = 4680 by = 63.78[sec]\n",
            "Finished mapping speaker id = 4788 by = 67.28[sec]\n",
            "Finished mapping speaker id = 481 by = 61.65[sec]\n",
            "Finished mapping speaker id = 4813 by = 66.92[sec]\n",
            "Finished mapping speaker id = 4830 by = 62.34[sec]\n",
            "Finished mapping speaker id = 4853 by = 65.51[sec]\n",
            "Finished mapping speaker id = 4859 by = 53.01[sec]\n",
            "Finished mapping speaker id = 4898 by = 67.61[sec]\n",
            "Finished mapping speaker id = 5022 by = 69.03[sec]\n",
            "Finished mapping speaker id = 5049 by = 62.59[sec]\n",
            "Finished mapping speaker id = 5104 by = 65.94[sec]\n",
            "Finished mapping speaker id = 5163 by = 61.21[sec]\n",
            "Finished mapping speaker id = 5192 by = 61.51[sec]\n",
            "Finished mapping speaker id = 5322 by = 66.88[sec]\n",
            "Finished mapping speaker id = 5339 by = 61.12[sec]\n",
            "Finished mapping speaker id = 5390 by = 64.65[sec]\n",
            "Finished mapping speaker id = 5393 by = 61.36[sec]\n",
            "Finished mapping speaker id = 5456 by = 47.12[sec]\n",
            "Finished mapping speaker id = 5463 by = 68.41[sec]\n",
            "Finished mapping speaker id = 5514 by = 42.90[sec]\n",
            "Finished mapping speaker id = 5561 by = 57.19[sec]\n",
            "Finished mapping speaker id = 5652 by = 60.41[sec]\n",
            "Finished mapping speaker id = 5678 by = 64.38[sec]\n",
            "Finished mapping speaker id = 5688 by = 61.54[sec]\n",
            "Finished mapping speaker id = 5703 by = 64.53[sec]\n",
            "Finished mapping speaker id = 5750 by = 63.44[sec]\n",
            "Finished mapping speaker id = 5778 by = 58.23[sec]\n",
            "Finished mapping speaker id = 5789 by = 66.11[sec]\n",
            "Finished mapping speaker id = 5808 by = 74.26[sec]\n",
            "Finished mapping speaker id = 5867 by = 61.17[sec]\n",
            "Finished mapping speaker id = 587 by = 66.13[sec]\n",
            "Finished mapping speaker id = 60 by = 51.06[sec]\n",
            "Finished mapping speaker id = 6000 by = 46.44[sec]\n",
            "Finished mapping speaker id = 6019 by = 65.99[sec]\n",
            "Finished mapping speaker id = 6064 by = 66.73[sec]\n",
            "Finished mapping speaker id = 6078 by = 61.15[sec]\n",
            "Finished mapping speaker id = 6081 by = 62.67[sec]\n",
            "Finished mapping speaker id = 6147 by = 64.29[sec]\n",
            "Finished mapping speaker id = 6181 by = 64.65[sec]\n",
            "Finished mapping speaker id = 6209 by = 67.53[sec]\n",
            "Finished mapping speaker id = 625 by = 65.68[sec]\n",
            "Finished mapping speaker id = 6272 by = 62.61[sec]\n",
            "Finished mapping speaker id = 6367 by = 62.60[sec]\n",
            "Finished mapping speaker id = 6385 by = 64.54[sec]\n",
            "Finished mapping speaker id = 6415 by = 65.58[sec]\n",
            "Finished mapping speaker id = 6437 by = 67.22[sec]\n",
            "Finished mapping speaker id = 6454 by = 65.79[sec]\n",
            "Finished mapping speaker id = 6476 by = 64.56[sec]\n",
            "Finished mapping speaker id = 6529 by = 66.63[sec]\n",
            "Finished mapping speaker id = 6531 by = 47.91[sec]\n",
            "Finished mapping speaker id = 6563 by = 57.85[sec]\n",
            "Finished mapping speaker id = 669 by = 65.35[sec]\n",
            "Finished mapping speaker id = 6818 by = 66.88[sec]\n",
            "Finished mapping speaker id = 6836 by = 64.31[sec]\n",
            "Finished mapping speaker id = 6848 by = 66.17[sec]\n",
            "Finished mapping speaker id = 6880 by = 65.33[sec]\n",
            "Finished mapping speaker id = 6925 by = 42.68[sec]\n",
            "Finished mapping speaker id = 696 by = 66.41[sec]\n",
            "Finished mapping speaker id = 7059 by = 64.96[sec]\n",
            "Finished mapping speaker id = 7067 by = 66.26[sec]\n",
            "Finished mapping speaker id = 7078 by = 64.05[sec]\n",
            "Finished mapping speaker id = 7113 by = 68.81[sec]\n",
            "Finished mapping speaker id = 7148 by = 67.15[sec]\n",
            "Finished mapping speaker id = 7178 by = 66.14[sec]\n",
            "Finished mapping speaker id = 7190 by = 60.01[sec]\n",
            "Finished mapping speaker id = 7226 by = 66.65[sec]\n",
            "Finished mapping speaker id = 7264 by = 60.74[sec]\n",
            "Finished mapping speaker id = 7278 by = 65.30[sec]\n",
            "Finished mapping speaker id = 730 by = 54.05[sec]\n",
            "Finished mapping speaker id = 7302 by = 60.18[sec]\n",
            "Finished mapping speaker id = 7312 by = 13.31[sec]\n",
            "Finished mapping speaker id = 7367 by = 64.33[sec]\n",
            "Finished mapping speaker id = 7402 by = 64.79[sec]\n",
            "Finished mapping speaker id = 7447 by = 65.27[sec]\n",
            "Finished mapping speaker id = 7505 by = 63.86[sec]\n",
            "Finished mapping speaker id = 7511 by = 67.17[sec]\n",
            "Finished mapping speaker id = 7517 by = 65.38[sec]\n",
            "Finished mapping speaker id = 7635 by = 63.02[sec]\n",
            "Finished mapping speaker id = 7780 by = 61.96[sec]\n",
            "Finished mapping speaker id = 7794 by = 69.52[sec]\n",
            "Finished mapping speaker id = 78 by = 64.55[sec]\n",
            "Finished mapping speaker id = 7800 by = 64.29[sec]\n",
            "Finished mapping speaker id = 7859 by = 52.77[sec]\n",
            "Finished mapping speaker id = 8014 by = 34.45[sec]\n",
            "Finished mapping speaker id = 8051 by = 65.15[sec]\n",
            "Finished mapping speaker id = 8063 by = 56.17[sec]\n",
            "Finished mapping speaker id = 8088 by = 64.53[sec]\n",
            "Finished mapping speaker id = 8095 by = 67.19[sec]\n",
            "Finished mapping speaker id = 8098 by = 66.86[sec]\n",
            "Finished mapping speaker id = 8108 by = 69.39[sec]\n",
            "Finished mapping speaker id = 8123 by = 65.41[sec]\n",
            "Finished mapping speaker id = 8226 by = 65.42[sec]\n",
            "Finished mapping speaker id = 8238 by = 64.91[sec]\n",
            "Finished mapping speaker id = 83 by = 64.16[sec]\n",
            "Finished mapping speaker id = 831 by = 63.00[sec]\n",
            "Finished mapping speaker id = 8312 by = 67.46[sec]\n",
            "Finished mapping speaker id = 8324 by = 66.97[sec]\n",
            "Finished mapping speaker id = 839 by = 48.34[sec]\n",
            "Finished mapping speaker id = 8419 by = 68.18[sec]\n",
            "Finished mapping speaker id = 8425 by = 66.40[sec]\n",
            "Finished mapping speaker id = 8465 by = 67.81[sec]\n",
            "Finished mapping speaker id = 8468 by = 65.52[sec]\n",
            "Finished mapping speaker id = 8580 by = 52.93[sec]\n",
            "Finished mapping speaker id = 8609 by = 65.74[sec]\n",
            "Finished mapping speaker id = 8629 by = 63.73[sec]\n",
            "Finished mapping speaker id = 8630 by = 62.09[sec]\n",
            "Finished mapping speaker id = 87 by = 68.11[sec]\n",
            "Finished mapping speaker id = 8747 by = 61.03[sec]\n",
            "Finished mapping speaker id = 8770 by = 72.94[sec]\n",
            "Finished mapping speaker id = 8797 by = 58.05[sec]\n",
            "Finished mapping speaker id = 8838 by = 67.49[sec]\n",
            "Finished mapping speaker id = 887 by = 65.57[sec]\n",
            "Finished mapping speaker id = 89 by = 64.24[sec]\n",
            "Finished mapping speaker id = 8975 by = 67.61[sec]\n",
            "Finished mapping speaker id = 909 by = 88.16[sec]\n",
            "{0: 103, 1: 1034, 2: 1040, 3: 1069, 4: 1081, 5: 1088, 6: 1098, 7: 1116, 8: 118, 9: 1183, 10: 1235, 11: 1246, 12: 125, 13: 1263, 14: 1334, 15: 1355, 16: 1363, 17: 1447, 18: 1455, 19: 150, 20: 1502, 21: 1553, 22: 1578, 23: 1594, 24: 1624, 25: 163, 26: 1723, 27: 1737, 28: 1743, 29: 1841, 30: 1867, 31: 1898, 32: 19, 33: 1926, 34: 196, 35: 1963, 36: 1970, 37: 198, 38: 1992, 39: 200, 40: 2002, 41: 2007, 42: 201, 43: 2092, 44: 211, 45: 2136, 46: 2159, 47: 2182, 48: 2196, 49: 226, 50: 2289, 51: 229, 52: 233, 53: 2384, 54: 2391, 55: 2416, 56: 2436, 57: 248, 58: 250, 59: 2514, 60: 2518, 61: 254, 62: 26, 63: 2691, 64: 27, 65: 2764, 66: 2817, 67: 2836, 68: 2843, 69: 289, 70: 2893, 71: 2910, 72: 2911, 73: 2952, 74: 298, 75: 2989, 76: 302, 77: 307, 78: 311, 79: 3112, 80: 3168, 81: 32, 82: 3214, 83: 322, 84: 3235, 85: 3240, 86: 3242, 87: 3259, 88: 328, 89: 332, 90: 3374, 91: 3436, 92: 3440, 93: 3486, 94: 3526, 95: 3607, 96: 3664, 97: 3699, 98: 3723, 99: 374, 100: 3807, 101: 3830, 102: 3857, 103: 3879, 104: 39, 105: 3947, 106: 3982, 107: 3983, 108: 40, 109: 4014, 110: 4018, 111: 403, 112: 405, 113: 4051, 114: 4088, 115: 412, 116: 4137, 117: 4160, 118: 4195, 119: 4214, 120: 426, 121: 4267, 122: 4297, 123: 4340, 124: 4362, 125: 4397, 126: 4406, 127: 441, 128: 4441, 129: 445, 130: 446, 131: 4481, 132: 458, 133: 460, 134: 4640, 135: 4680, 136: 4788, 137: 481, 138: 4813, 139: 4830, 140: 4853, 141: 4859, 142: 4898, 143: 5022, 144: 5049, 145: 5104, 146: 5163, 147: 5192, 148: 5322, 149: 5339, 150: 5390, 151: 5393, 152: 5456, 153: 5463, 154: 5514, 155: 5561, 156: 5652, 157: 5678, 158: 5688, 159: 5703, 160: 5750, 161: 5778, 162: 5789, 163: 5808, 164: 5867, 165: 587, 166: 60, 167: 6000, 168: 6019, 169: 6064, 170: 6078, 171: 6081, 172: 6147, 173: 6181, 174: 6209, 175: 625, 176: 6272, 177: 6367, 178: 6385, 179: 6415, 180: 6437, 181: 6454, 182: 6476, 183: 6529, 184: 6531, 185: 6563, 186: 669, 187: 6818, 188: 6836, 189: 6848, 190: 6880, 191: 6925, 192: 696, 193: 7059, 194: 7067, 195: 7078, 196: 7113, 197: 7148, 198: 7178, 199: 7190, 200: 7226, 201: 7264, 202: 7278, 203: 730, 204: 7302, 205: 7312, 206: 7367, 207: 7402, 208: 7447, 209: 7505, 210: 7511, 211: 7517, 212: 7635, 213: 7780, 214: 7794, 215: 78, 216: 7800, 217: 7859, 218: 8014, 219: 8051, 220: 8063, 221: 8088, 222: 8095, 223: 8098, 224: 8108, 225: 8123, 226: 8226, 227: 8238, 228: 83, 229: 831, 230: 8312, 231: 8324, 232: 839, 233: 8419, 234: 8425, 235: 8465, 236: 8468, 237: 8580, 238: 8609, 239: 8629, 240: 8630, 241: 87, 242: 8747, 243: 8770, 244: 8797, 245: 8838, 246: 887, 247: 89, 248: 8975, 249: 909, 250: 911}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-84c71ef18ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#Write mapping to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'speakers_mapping.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSt1NhM8zTng"
      },
      "source": [
        "##Get Number Of Speakers (subdirectories)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZa5QOZrycaY",
        "outputId": "aed9aa08-e88b-4e81-f8b7-6612dbdd2601"
      },
      "source": [
        "num_speakers = len(list(os.walk('/content/drive/MyDrive/DeepProject/wav2vec/SV_Librispeech_Latent_Dataset_new/')))\r\n",
        "print(num_speakers)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "234ticIOtFyy"
      },
      "source": [
        "##Print total number of speakers audio dataset after cutting below 3 seconds examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvU1nDRvqa2F",
        "outputId": "301bef9c-704f-4d8a-dea8-0e9b36ba3d39"
      },
      "source": [
        "print(len(list(os.walk('/content/drive/MyDrive/DeepProject/wav2vec/SV_Librispeech_Dataset/'))))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7I8R5VKOuYD"
      },
      "source": [
        "##Write list of audio lengths to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZfJGaKbOy0-",
        "outputId": "7510c240-abc4-450d-f4d5-ceaa220c38ff"
      },
      "source": [
        "from tabulate import tabulate\r\n",
        "x = list(range(25))\r\n",
        "l = [0, 24, 563, 683, 670, 620, 636, 661, 722, 785, 975, 1379, 2491, 4074, 6012, 6428, 1763, 49, 1, 2, 0, 0, 0, 0, 1]\r\n",
        "table_data = [x, l]\r\n",
        "print(tabulate(table_data, headers='firstrow', tablefmt='fancy_grid'))\r\n",
        "#write it to a file\r\n",
        "with open('histogram.txt', 'w') as f:\r\n",
        "  f.write(tabulate(table_data, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "╒═════╤═════╤═════╤═════╤═════╤═════╤═════╤═════╤═════╤═════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╕\n",
            "│   0 │   1 │   2 │   3 │   4 │   5 │   6 │   7 │   8 │   9 │   10 │   11 │   12 │   13 │   14 │   15 │   16 │   17 │   18 │   19 │   20 │   21 │   22 │   23 │   24 │\n",
            "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╡\n",
            "│   0 │  24 │ 563 │ 683 │ 670 │ 620 │ 636 │ 661 │ 722 │ 785 │  975 │ 1379 │ 2491 │ 4074 │ 6012 │ 6428 │ 1763 │   49 │    1 │    2 │    0 │    0 │    0 │    0 │    1 │\n",
            "╘═════╧═════╧═════╧═════╧═════╧═════╧═════╧═════╧═════╧═════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╛\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58OZqHFLQ2w"
      },
      "source": [
        "##Just checking it worked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8WkMdWxKSmh",
        "outputId": "2a2a5e69-b2b0-499c-f9ec-125486a4fcf0"
      },
      "source": [
        "%%time\r\n",
        "counter = 0\r\n",
        "for i, data in enumerate(my_train_loader, 0):\r\n",
        "  counter = counter + 1\r\n",
        "  if counter > 3:\r\n",
        "    break\r\n",
        "  with torch.no_grad():\r\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\r\n",
        "    print(inputs[0].shape)\r\n",
        "    print(inputs[batch_size-1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "CPU times: user 145 ms, sys: 78.4 ms, total: 223 ms\n",
            "Wall time: 30.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCmTGT_Nxckv"
      },
      "source": [
        "##Check size of preprocessed data (no repr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QM5NPCNwhl7",
        "outputId": "904be388-7e48-45f1-af12-42dbe045744f"
      },
      "source": [
        "a, s1 = torchaudio.load('/content/drive/MyDrive/DeepProject/wav2vec/SV_Librispeech_Dataset/1088/5.flac')\r\n",
        "b, s2 = torchaudio.load('/content/drive/MyDrive/DeepProject/wav2vec/SV_Librispeech_Dataset/1088/6.flac')\r\n",
        "print(a.shape, s1)\r\n",
        "print(b.shape, s2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyDrive  Shareddrives\n",
            "torch.Size([1, 48000]) 16000\n",
            "torch.Size([1, 48000]) 16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4d8zcXnI-PH"
      },
      "source": [
        "##Show Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "cl6hfTJl30r0",
        "outputId": "3307defc-1e5f-4829-80b9-94e2b4fe2ffa"
      },
      "source": [
        "waveform, sample_rate = torchaudio.load(f'{speaker_84_data_dir}/121123/84-121123-0003.flac')\r\n",
        "\r\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))\r\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(waveform.t().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of waveform: torch.Size([1, 108800])\n",
            "Sample rate of waveform: 16000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f555f8462b0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KzliWhmM4z"
      },
      "source": [
        "###Get information about audio file include the duration time in seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKQpj0VElfL0",
        "outputId": "60154bb9-0fc9-46bb-de0f-5ba9c3fa684b"
      },
      "source": [
        "print(sf.info(f'{speaker_84_data_dir}/121123/84-121123-0003.flac'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DeepProject/wav2vec/LibriSpeech/dev-clean/84/121123/84-121123-0003.flac\n",
            "samplerate: 16000 Hz\n",
            "channels: 1\n",
            "duration: 6.800 s\n",
            "format: FLAC (Free Lossless Audio Codec) [FLAC]\n",
            "subtype: Signed 16 bit PCM [PCM_16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8j9j6HDwjtq"
      },
      "source": [
        "##On Top Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxdGfXSAwrmv"
      },
      "source": [
        "class NAIVE_SV(nn.Module):\r\n",
        "    '''This class represents a model that can take as input a [512,width]\r\n",
        "      (width -> ?[sec] uuterance) and outputs a softmax for speaker verification\r\n",
        "       task.\r\n",
        "       Model Architecture:\r\n",
        "       1. Avg pool on whole time axis (298 samples)\r\n",
        "       2. Linear transformation that will output as number of speakers'''\r\n",
        "\r\n",
        "    def __init__(self, width, speakers_num):\r\n",
        "        super(NAIVE_SV, self).__init__()\r\n",
        "        self._height = 512  # according wav2vec last conv layer kernels amount\r\n",
        "        self._width = width\r\n",
        "        self.speakers_num = speakers_num\r\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=self._width)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(self._height, self.speakers_num),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "        # Dont think we need any regularization for this thin network but just in case:\r\n",
        "        # self.batch_norm = nn.BatchNorm1d(self.speakers_num)\r\n",
        "        # self.drop = nn.Dropout(0.5)  #Hard coded probability for now...\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # Forward pass\r\n",
        "        x = self.avg_pool(x)\r\n",
        "        print(x.shape)\r\n",
        "        x = x.view(-1, self._height)\r\n",
        "        print(x.shape)\r\n",
        "        x = self.fc(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "# maybe for future use:\r\n",
        "class SIAMESE_SV(nn.Module):\r\n",
        "    def __init__(self, width, speakers_num):\r\n",
        "        super(SIAMESE_SV, self).__init__()\r\n",
        "        self._height = self._height  # according wav2vec last conv layer kernels amount\r\n",
        "        self._width = width\r\n",
        "        self.speakers_num = speakers_num\r\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=self._width)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(self._height, self.speakers_num),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        def forward_once(self, x):\r\n",
        "            # Forward pass\r\n",
        "            x = self.avg_pool(x)\r\n",
        "            x = x.view(-1, self._height)\r\n",
        "            x = self.fc(x)\r\n",
        "            return x\r\n",
        "\r\n",
        "        def forward(self, input1, input2):\r\n",
        "            # forward pass of input 1\r\n",
        "            output1 = self.forward_once(input1)\r\n",
        "            # forward pass of input 2\r\n",
        "            output2 = self.forward_once(input2)\r\n",
        "            # returning the feature vectors of two inputs\r\n",
        "            return output1, output2\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01WRFFUYkdXJ"
      },
      "source": [
        "##Contrastive Loss Definition (future use)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5P15IWNkggE"
      },
      "source": [
        "class ContrastiveLoss(nn.Module):\r\n",
        "  def __init__(self, margin=2.0):\r\n",
        "    super(ContrastiveLoss, self).__init__()\r\n",
        "    self.margin = margin\r\n",
        "  \r\n",
        "  \r\n",
        "  def forward(self, output1, output2, label):\r\n",
        "    # Find the pairwise distance or eucledian distance of two output feature vectors\r\n",
        "    euclidean_distance = F.pairwise_distance(output1, output2)  #maybe use another distance like cosine similarity function\r\n",
        "    # perform contrastive loss calculation with the distance\r\n",
        "    loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\r\n",
        "    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\r\n",
        "    return loss_contrastive"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRMUgnPQh06k"
      },
      "source": [
        "##Declare Model, Criterion and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyzaaNtbh9sF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf89dee-b79c-4ebd-a13d-22faeff1a2c3"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #setting GPU device\r\n",
        "print(device)\r\n",
        "net = NAIVE_SV(width, num_speakers).to(device) #declare model\r\n",
        "criterion = nn.CrossEntropyLoss() #Combines nn.LogSoftmax() & nn.NLLLoss\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)  #need to check what the wav2vec2 paper did with the learning rate (i think it was frozen for big amount of steps and afterwards updated each step)\r\n",
        "print(net)\r\n",
        "\r\n",
        "#just checking it out:\r\n",
        "net.to(device)\r\n",
        "net.train()\r\n",
        "a, l = next(iter(my_train_loader))\r\n",
        "print(a.shape)\r\n",
        "a = a.to(device)\r\n",
        "l = l.to(device)\r\n",
        "print(l.shape)\r\n",
        "outputs = net(a)\r\n",
        "print(outputs.shape)\r\n",
        "loss = criterion(outputs, l)\r\n",
        "_, top_class = outputs.topk(1, dim=1)\r\n",
        "print(f'The speakers are {l[:6]}')\r\n",
        "print(f'The net outputs are {top_class[:6]}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "NAIVE_SV(\n",
            "  (avg_pool): AvgPool1d(kernel_size=(298,), stride=(298,), padding=(0,))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=190, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n",
            "torch.Size([32, 512, 298])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 512, 1])\n",
            "torch.Size([32, 512])\n",
            "torch.Size([32, 190])\n",
            "The speakers are tensor([118,  84,  16, 154, 115,  28], device='cuda:0')\n",
            "The net outputs are tensor([[167],\n",
            "        [ 65],\n",
            "        [ 10],\n",
            "        [164],\n",
            "        [164],\n",
            "        [ 65]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mle8wZjCh-KF"
      },
      "source": [
        "##Classfication Generic Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oKSikRdiDho"
      },
      "source": [
        "#TODO:\r\n",
        "# Create loss list and present it to screen during training\r\n",
        "# Write an evaluation function for the classification task\r\n",
        "\r\n",
        "%%time\r\n",
        "#Loop over epochs\r\n",
        "for epoch in range(epochs):\r\n",
        "  #moving model to GPU & keeping the network in training mode\r\n",
        "  net.to(device)\r\n",
        "  net.train()\r\n",
        "  #iterate through all the batches in each epoch\r\n",
        "  for i, data in enumerate(train_loader, 0):\r\n",
        "    #get inputs and labels and move them to gpu\r\n",
        "    inputs, labels = data[0].to(device), data[1] .to(device)\r\n",
        "    #clear the gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    #make a forward pass\r\n",
        "    outputs = net(inputs)\r\n",
        "    #calculate loss\r\n",
        "    loss = criterion(outputs, labels)\r\n",
        "    #backward pass\r\n",
        "    loss.backward()\r\n",
        "    #perform a single optimization step (parameter update)\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  #Validation:\r\n",
        "  # for i, data in enumerate(train_loader, 0):\r\n",
        "  #     #validate the model\r\n",
        "  #     with torch.no_grad():\r\n",
        "  #       net.eval()\r\n",
        "  #       inputs, labels = data[0].to(device), data[1] .to(device)\r\n",
        "  #       outputs = net(inputs) \r\n",
        "  #       #get the most likely label predicted\r\n",
        "  #       _, top_class = outputs.topk(1, dim=1)\r\n",
        "  #       #get number of matches with the real labels\r\n",
        "  #       equals = top_class == labels.view(*top_class.shape)\r\n",
        "  #       train_accuracy[reg_index, epoch] += torch.sum(equals.type(torch.FloatTensor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTIoEznNtUGo"
      },
      "source": [
        "##Siamese Generic Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35AyzqrUtV6B"
      },
      "source": [
        "%%time\r\n",
        "net = SIAMESE_SV.to(device)\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}