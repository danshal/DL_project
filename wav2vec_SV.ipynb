{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wav2vec_SV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1KQCKSvW-6w1L9FBhYF6amMsdtBm79JEO",
      "authorship_tag": "ABX9TyMlKNt+ltwineMVTqFLUp7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d44f8ec8194f407cab123363877c38bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_79e82e8e94e94a0c9b6c636f18aa579e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1ce7c79f5f23432092102b45bb109911",
              "IPY_MODEL_3c6073f61b094b2ba761f6d317ea9ab5"
            ]
          }
        },
        "79e82e8e94e94a0c9b6c636f18aa579e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ce7c79f5f23432092102b45bb109911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e3273e8503a546bf8d8c25884ef78f89",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 346663984,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 346663984,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa546b610074426897b4730940ae59cc"
          }
        },
        "3c6073f61b094b2ba761f6d317ea9ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_541cc495ce084bc8ba5b9be3bd893559",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 331M/331M [00:55&lt;00:00, 6.23MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06333d445cee4bfda4055a4162ea2ae8"
          }
        },
        "e3273e8503a546bf8d8c25884ef78f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa546b610074426897b4730940ae59cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "541cc495ce084bc8ba5b9be3bd893559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06333d445cee4bfda4055a4162ea2ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danshal/DL_project/blob/develop/wav2vec_SV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wl_onLCsnYA"
      },
      "source": [
        "##Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av8CcywGscnF",
        "outputId": "a09f2e14-d336-48f9-c3c6-9cf98e78cbf6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfi2rXAIs9rR"
      },
      "source": [
        "##Install fairseq and more packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMEDrdPctBfp",
        "outputId": "1514ee25-a825-4b93-8656-b3f6c4f75b40"
      },
      "source": [
        "!pip install soundfile git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\r\n",
        "!pip install fairseq\r\n",
        "!pip install torchaudio\r\n",
        "#!mkdir fairseq_code\r\n",
        "#%cd /content/drive/MyDrive/DeepProject/wav2vec/fairseq_code/\r\n",
        "#!git clone https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\n",
            "  Cloning git://github.com/pytorch/fairseq.git (to revision b8ea8a9b72c82192da07e3377adf4ebbde16716d) to /tmp/pip-req-build-pgso7tzk\n",
            "  Running command git clone -q git://github.com/pytorch/fairseq.git /tmp/pip-req-build-pgso7tzk\n",
            "  Running command git checkout -q b8ea8a9b72c82192da07e3377adf4ebbde16716d\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (2019.12.20)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (0.29.21)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.7.0+cu101)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.14.4)\n",
            "Collecting hydra-core<1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6e/6298e4099ecf7344fb6621e83ec92ba4384699397b2d9b2d17819f869a1d/hydra_core-1.0.5-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 15.2MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (4.41.1)\n",
            "Requirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==1.0.0a0+b8ea8a9) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==1.0.0a0+b8ea8a9) (3.7.4.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==1.0.0a0+b8ea8a9) (2.20)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (5.1.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 32.4MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/28/94/690880aa6cd96130fc0abf82ee8087b8de2d3c55515a3e42793c58d8a353/portalocker-2.1.0-py2.py3-none-any.whl\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (3.4.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+b8ea8a9-cp36-cp36m-linux_x86_64.whl size=2713490 sha256=ede2c4c05afaaac695e3274ecf18f1ef48db3457ed399db34876beb84bed643c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oimj1spi/wheels/39/26/93/08a9f837c7e94436027d116b1ffe57822725b2caf742649fb3\n",
            "Successfully built fairseq\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141231 sha256=a059c6008605c464dc1fc97b1af24e886d6e3c1fcd06c7f918489be3a2f74411\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, portalocker, sacrebleu, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq-1.0.0a0+b8ea8a9 hydra-core-1.0.5 omegaconf-2.0.6 portalocker-2.1.0 sacrebleu-1.5.0\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (1.0.0a0+b8ea8a9)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: hydra-core<1.1 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.0.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
            "Requirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq) (5.4.1)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq) (5.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq) (4.8)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aagqz11s7f1"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk5-AwDdtm9o",
        "outputId": "bf37220c-f799-4625-cd64-96cb53778deb"
      },
      "source": [
        "#General imports\r\n",
        "import argparse\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import os.path as osp\r\n",
        "import pprint\r\n",
        "import soundfile as sf\r\n",
        "#facebook team framework\r\n",
        "import fairseq\r\n",
        "#pytorch\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchaudio\r\n",
        "#matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "try:\r\n",
        "    import tqdm\r\n",
        "except:\r\n",
        "    print(\"Install tqdm to use --log-format=tqdm\")\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
            "  '\"sox\" backend is being deprecated. '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVmPE6q7t_Eq"
      },
      "source": [
        "##Try sanity code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nedhgxmcuDaS",
        "outputId": "a04df68e-595b-49c9-99be-7d56e70174fe"
      },
      "source": [
        "import torch\r\n",
        "import fairseq\r\n",
        "\r\n",
        "cp_path = cp_path = '/content/drive/My Drive/DeepProject/wav2vec/wav2vec_large.pt'\r\n",
        "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n",
        "model = model[0]\r\n",
        "model.eval()\r\n",
        "\r\n",
        "wav_input_16khz = torch.randn(1,10000)\r\n",
        "z = model.feature_extractor(wav_input_16khz)\r\n",
        "c = model.feature_aggregator(z)\r\n",
        "print(f'input shape = {wav_input_16khz.shape} ; z shape = {z.shape} ; z shape = {c.shape}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape = torch.Size([1, 10000]) ; z shape = torch.Size([1, 512, 60]) ; z shape = torch.Size([1, 512, 60])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euSpEw7Z3QT0"
      },
      "source": [
        "#@title Hyperparametes\n",
        "batch_size = 32\n",
        "epochs = 100\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-sCUryI1ptJ"
      },
      "source": [
        "###Download Dataset\r\n",
        "###Information about LibriSpeech dataset:\r\n",
        "1. Total of 982 hours of 16KHz read English speech\r\n",
        "2. 2484 speakers in this corpus.\r\n",
        "3. Suppose to be reasonably balances in terms of gender and per-speaker duration\r\n",
        "4. Allowed urls for LIBRISPEECH - \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\", \"train-clean-100\", \"train-clean-360\" and \"train-other-500\". (default: \"train-clean-100\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d44f8ec8194f407cab123363877c38bd",
            "79e82e8e94e94a0c9b6c636f18aa579e",
            "1ce7c79f5f23432092102b45bb109911",
            "3c6073f61b094b2ba761f6d317ea9ab5",
            "e3273e8503a546bf8d8c25884ef78f89",
            "fa546b610074426897b4730940ae59cc",
            "541cc495ce084bc8ba5b9be3bd893559",
            "06333d445cee4bfda4055a4162ea2ae8"
          ]
        },
        "id": "nU6KWwI91ruB",
        "outputId": "e112d7e3-e7a0-4851-b39d-cbd3eab5138e"
      },
      "source": [
        "rootDir = '/content/drive/MyDrive/DeepProject/wav2vec/'\r\n",
        "# Download and load the training data\r\n",
        "train_data = torchaudio.datasets.LIBRISPEECH(f'{rootDir}/dataset/', download=True)\r\n",
        "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\r\n",
        "# Download and load the test data\r\n",
        "test_data = torchaudio.datasets.LIBRISPEECH(f'{rootDir}/dataset/', url='test-clean', download=True)\r\n",
        "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d44f8ec8194f407cab123363877c38bd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "qG1VekgxMwkp",
        "outputId": "c0004c91-1caf-4eb3-8294-479f396568cb"
      },
      "source": [
        "import os\r\n",
        "from typing import Tuple\r\n",
        "\r\n",
        "import torchaudio\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchaudio.datasets.utils import (\r\n",
        "    download_url,\r\n",
        "    extract_archive,\r\n",
        "    walk_files,\r\n",
        ")\r\n",
        "\r\n",
        "URL = \"train-clean-100\"\r\n",
        "FOLDER_IN_ARCHIVE = \"LibriSpeech\"\r\n",
        "_CHECKSUMS = {\r\n",
        "    \"http://www.openslr.org/resources/12/dev-clean.tar.gz\":\r\n",
        "    \"76f87d090650617fca0cac8f88b9416e0ebf80350acb97b343a85fa903728ab3\",\r\n",
        "    \"http://www.openslr.org/resources/12/dev-other.tar.gz\":\r\n",
        "    \"12661c48e8c3fe1de2c1caa4c3e135193bfb1811584f11f569dd12645aa84365\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-clean.tar.gz\":\r\n",
        "    \"39fde525e59672dc6d1551919b1478f724438a95aa55f874b576be21967e6c23\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-other.tar.gz\":\r\n",
        "    \"d09c181bba5cf717b3dee7d4d592af11a3ee3a09e08ae025c5506f6ebe961c29\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\":\r\n",
        "    \"d4ddd1d5a6ab303066f14971d768ee43278a5f2a0aa43dc716b0e64ecbbbf6e2\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\":\r\n",
        "    \"146a56496217e96c14334a160df97fffedd6e0a04e66b9c5af0d40be3c792ecf\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-other-500.tar.gz\":\r\n",
        "    \"ddb22f27f96ec163645d53215559df6aa36515f26e01dd70798188350adcb6d2\"\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def load_librispeech_item(fileid: str,\r\n",
        "                          path: str,\r\n",
        "                          ext_audio: str) -> Tuple[Tensor, int]:\r\n",
        "    speaker_id\r\n",
        "\r\n",
        "    # file_text = speaker_id + \"-\" + chapter_id + ext_txt\r\n",
        "    # file_text = os.path.join(path, speaker_id, chapter_id, file_text)\r\n",
        "\r\n",
        "    fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\r\n",
        "    file_audio = fileid_audio + ext_audio\r\n",
        "    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\r\n",
        "\r\n",
        "    # Load audio - don't care about sample rate\r\n",
        "    waveform, _ = torchaudio.load(file_audio)\r\n",
        "\r\n",
        "    return (waveform, int(speaker_id))\r\n",
        "\r\n",
        "\r\n",
        "class SV_LIBRISPEECH(torchaudio.datasets.LIBRISPEECH):\r\n",
        "  def __init__(self, root: str, url: str = 'train-clean-100', folder_in_archive: str = 'SV_LibriSpeech', download: bool = False):\r\n",
        "    super().__init__(root, download)\r\n",
        "\r\n",
        "\r\n",
        "  def __getitem__(self, n: int) -> Tuple[Tensor, int]:\r\n",
        "        \"\"\"Load the n-th sample from the dataset.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            n (int): The index of the sample to be loaded\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple: ``(waveform, speaker_id)``\r\n",
        "        \"\"\"\r\n",
        "        fileid = self._walker[n]\r\n",
        "        return load_librispeech_item(fileid, self._path, self._ext_audio)\r\n",
        "\r\n",
        "  def __len__(self) -> int:\r\n",
        "    return len(self._walker)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "my_train_data = SV_LIBRISPEECH(f'{rootDir}dataset/', download=True)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-02d59dfb2718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mmy_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSV_LIBRISPEECH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{rootDir}dataset/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-02d59dfb2718>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSV_LIBRISPEECH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLIBRISPEECH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train-clean-100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_in_archive\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SV_LibriSpeech'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchaudio/datasets/librispeech.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mbasename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mbasename\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m\"\"\"Returns the final component of a pathname\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not bool"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2us3hu3mUloM",
        "outputId": "c823290c-0fc0-46bb-bbef-5352131de3ed"
      },
      "source": [
        "print(len(my_train_data))\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "ExinD0y3S9pL",
        "outputId": "9fc1248f-e95b-4d52-cec8-396f777cd3ec"
      },
      "source": [
        "rootDir = '/content/drive/MyDrive/DeepProject/wav2vec/'\r\n",
        "\r\n",
        "import os\r\n",
        "from typing import Tuple\r\n",
        "\r\n",
        "import torchaudio\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchaudio.datasets.utils import (\r\n",
        "    download_url,\r\n",
        "    extract_archive,\r\n",
        "    walk_files,\r\n",
        ")\r\n",
        "\r\n",
        "URL = \"train-clean-100\"\r\n",
        "FOLDER_IN_ARCHIVE = \"LibriSpeech\"\r\n",
        "_CHECKSUMS = {\r\n",
        "    \"http://www.openslr.org/resources/12/dev-clean.tar.gz\":\r\n",
        "    \"76f87d090650617fca0cac8f88b9416e0ebf80350acb97b343a85fa903728ab3\",\r\n",
        "    \"http://www.openslr.org/resources/12/dev-other.tar.gz\":\r\n",
        "    \"12661c48e8c3fe1de2c1caa4c3e135193bfb1811584f11f569dd12645aa84365\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-clean.tar.gz\":\r\n",
        "    \"39fde525e59672dc6d1551919b1478f724438a95aa55f874b576be21967e6c23\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-other.tar.gz\":\r\n",
        "    \"d09c181bba5cf717b3dee7d4d592af11a3ee3a09e08ae025c5506f6ebe961c29\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\":\r\n",
        "    \"d4ddd1d5a6ab303066f14971d768ee43278a5f2a0aa43dc716b0e64ecbbbf6e2\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\":\r\n",
        "    \"146a56496217e96c14334a160df97fffedd6e0a04e66b9c5af0d40be3c792ecf\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-other-500.tar.gz\":\r\n",
        "    \"ddb22f27f96ec163645d53215559df6aa36515f26e01dd70798188350adcb6d2\"\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def load_librispeech_item(fileid: str,\r\n",
        "                          path: str,\r\n",
        "                          ext_audio: str,\r\n",
        "                          ext_txt: str) -> Tuple[Tensor, int]:\r\n",
        "    speaker_id\r\n",
        "\r\n",
        "    file_text = speaker_id + \"-\" + chapter_id + ext_txt\r\n",
        "    file_text = os.path.join(path, speaker_id, chapter_id, file_text)\r\n",
        "\r\n",
        "    fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\r\n",
        "    file_audio = fileid_audio + ext_audio\r\n",
        "    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\r\n",
        "\r\n",
        "    # Load audio\r\n",
        "    waveform, sample_rate = torchaudio.load(file_audio)\r\n",
        "\r\n",
        "    # Load text\r\n",
        "    with open(file_text) as ft:\r\n",
        "        for line in ft:\r\n",
        "            fileid_text, utterance = line.strip().split(\" \", 1)\r\n",
        "            if fileid_audio == fileid_text:\r\n",
        "                break\r\n",
        "        else:\r\n",
        "            # Translation not found\r\n",
        "            raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\r\n",
        "\r\n",
        "    return (waveform, int(speaker_id))\r\n",
        "\r\n",
        "\r\n",
        "class LIBRISPEECH_SV(Dataset):\r\n",
        "    \"\"\"Create a Dataset for LibriSpeech.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (str): Path to the directory where the dataset is found or downloaded.\r\n",
        "        url (str, optional): The URL to download the dataset from,\r\n",
        "            or the type of the dataset to dowload.\r\n",
        "            Allowed type values are ``\"dev-clean\"``, ``\"dev-other\"``, ``\"test-clean\"``,\r\n",
        "            ``\"test-other\"``, ``\"train-clean-100\"``, ``\"train-clean-360\"`` and\r\n",
        "            ``\"train-other-500\"``. (default: ``\"train-clean-100\"``)\r\n",
        "        folder_in_archive (str, optional):\r\n",
        "            The top-level directory of the dataset. (default: ``\"LibriSpeech\"``)\r\n",
        "        download (bool, optional):\r\n",
        "            Whether to download the dataset if it is not found at root path. (default: ``False``).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    _ext_txt = \".trans.txt\"\r\n",
        "    _ext_audio = \".flac\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 root: str,\r\n",
        "                 url: str = URL,\r\n",
        "                 folder_in_archive: str = FOLDER_IN_ARCHIVE,\r\n",
        "                 download: bool = False) -> None:\r\n",
        "\r\n",
        "        if url in [\r\n",
        "            \"dev-clean\",\r\n",
        "            \"dev-other\",\r\n",
        "            \"test-clean\",\r\n",
        "            \"test-other\",\r\n",
        "            \"train-clean-100\",\r\n",
        "            \"train-clean-360\",\r\n",
        "            \"train-other-500\",\r\n",
        "        ]:\r\n",
        "\r\n",
        "            ext_archive = \".tar.gz\"\r\n",
        "            base_url = \"http://www.openslr.org/resources/12/\"\r\n",
        "\r\n",
        "            url = os.path.join(base_url, url + ext_archive)\r\n",
        "\r\n",
        "        print(root, url, folder_in_archive, download)\r\n",
        "        basename = os.path.basename(url)\r\n",
        "        archive = os.path.join(root, basename)\r\n",
        "\r\n",
        "        basename = basename.split(\".\")[0]\r\n",
        "        folder_in_archive = os.path.join(folder_in_archive, basename)\r\n",
        "\r\n",
        "        self._path = os.path.join(root, folder_in_archive)\r\n",
        "\r\n",
        "        if download:\r\n",
        "            if not os.path.isdir(self._path):\r\n",
        "                if not os.path.isfile(archive):\r\n",
        "                    checksum = _CHECKSUMS.get(url, None)\r\n",
        "                    download_url(url, root, hash_value=checksum)\r\n",
        "                extract_archive(archive)\r\n",
        "\r\n",
        "        walker = walk_files(\r\n",
        "            self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\r\n",
        "        )\r\n",
        "        self._walker = list(walker)\r\n",
        "\r\n",
        "    def __getitem__(self, n: int) -> Tuple[Tensor, int]:\r\n",
        "        \"\"\"Load the n-th sample from the dataset.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            n (int): The index of the sample to be loaded\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple: ``(waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)``\r\n",
        "        \"\"\"\r\n",
        "        fileid = self._walker[n]\r\n",
        "        return load_librispeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self._walker)\r\n",
        "\r\n",
        "s = f'{rootDir}/dataset/'\r\n",
        "print(type(s))\r\n",
        "my_train_data = SV_LIBRISPEECH(f'{rootDir}/dataset/', download=True)\r\n",
        "print(len(my_train_data))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-dba2ebb258ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{rootDir}/dataset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mmy_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSV_LIBRISPEECH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{rootDir}/dataset/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-02d59dfb2718>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSV_LIBRISPEECH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLIBRISPEECH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train-clean-100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_in_archive\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SV_LibriSpeech'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchaudio/datasets/librispeech.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mbasename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mbasename\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m\"\"\"Returns the final component of a pathname\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not bool"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4d8zcXnI-PH"
      },
      "source": [
        "##Show Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxXLL63s9o8z",
        "outputId": "d7ddcc0c-00da-42f2-8f57-25cf8d3e7193"
      },
      "source": [
        "images = next(iter(train_data))\r\n",
        "count = 0\r\n",
        "for i in iter(train_data):\r\n",
        "  count = count + 1\r\n",
        "  print(i)\r\n",
        "  if count == 2:\r\n",
        "    break\r\n",
        "print(images[0].shape)  #audio samples as tensor\r\n",
        "print(images[3])  #represents speaker id"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[-0.0065, -0.0055, -0.0062,  ...,  0.0033,  0.0005, -0.0095]]), 16000, 'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK', 103, 1240, 0)\n",
            "(tensor([[-0.0059, -0.0045, -0.0067,  ...,  0.0007,  0.0034,  0.0047]]), 16000, \"THAT HAD ITS SOURCE AWAY BACK IN THE WOODS OF THE OLD CUTHBERT PLACE IT WAS REPUTED TO BE AN INTRICATE HEADLONG BROOK IN ITS EARLIER COURSE THROUGH THOSE WOODS WITH DARK SECRETS OF POOL AND CASCADE BUT BY THE TIME IT REACHED LYNDE'S HOLLOW IT WAS A QUIET WELL CONDUCTED LITTLE STREAM\", 103, 1240, 1)\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([1, 225360])\n",
            "103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "KhBOSECp_eGy",
        "outputId": "70a356a2-dc40-49ba-dcf7-ca9771dded00"
      },
      "source": [
        "for i, data in enumerate(train_loader, 0):\r\n",
        "  inputs, labels = data[0].to(device), data[1].to(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f80ab4e6317a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 255759] at entry 0 and [1, 205600] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl6hfTJl30r0"
      },
      "source": [
        "waveform, sample_rate = torchaudio.load(speaker_84_data_dir)\r\n",
        "\r\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))\r\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(waveform.t().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSGQC7gsuHU8"
      },
      "source": [
        "##Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "oB0XGoUAuRVK",
        "outputId": "ae99225f-c870-4e74-8a26-55686a35b3a0"
      },
      "source": [
        "speaker_84_data_dir = '/content/drive/MyDrive/DeepProject/wav2vec/LibriSpeech/dev-clean/84/'\r\n",
        "\r\n",
        "def read_audio(fname):\r\n",
        "    ''' Load an audio file and return PCM along with the sample rate '''\r\n",
        "\r\n",
        "    wav, sr = sf.read(fname)\r\n",
        "    assert sr == 16e3\r\n",
        "\r\n",
        "    return wav, 16e3\r\n",
        "\r\n",
        "\r\n",
        "def audio2tensor(fname):\r\n",
        "  ''' This function will convert audio samples to tensor '''\r\n",
        "  input, _ = read_audio(fname)\r\n",
        "  input_tensor = torch.from_numpy(input).float()\r\n",
        "  return input_tensor.reshape(1, input_tensor.size(0))\r\n",
        "\r\n",
        "\r\n",
        "def get_audio_repr(fname):\r\n",
        "  ''' This function will get audio as input and will output its representation\r\n",
        "      as wav2vec model outputs '''\r\n",
        "  input_tensor = audio2tensor(fname)\r\n",
        "  z = model.feature_extractor(input_tensor)\r\n",
        "  c = model.feature_aggregator(z)\r\n",
        "  return c\r\n",
        "\r\n",
        "\r\n",
        "#def preprocess_data(dataset_folder):\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "input1_tensor = audio2tensor(f'{speaker_84_data_dir}/121123/84-121123-0003.flac')\r\n",
        "input2_tensor = audio2tensor(f'{speaker_84_data_dir}/121123/84-121123-0004.flac')\r\n",
        "input1_repr = get_audio_repr(f'{speaker_84_data_dir}/121123/84-121123-0003.flac')\r\n",
        "print(f'input1_tensor shape = {input1_tensor.shape} ; input2_tensor shape = {input2_tensor.shape}')\r\n",
        "print(f'input1 repr = {input1_repr} with shape = {input1_repr.shape}')\r\n",
        "print(input1_tensor.shape[1] / input1_repr.shape[2])\r\n",
        "#Get whole models output\r\n",
        "# a = model(input2_tensor)\r\n",
        "# print(a)\r\n",
        "# print(a['cpc_targets'].shape)\r\n",
        "# for i in a['cpc_targets']:\r\n",
        "#   if i != torch.tensor(0):\r\n",
        "#     print('OK')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2b6e059170bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspeaker_84_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/DeepProject/wav2vec/LibriSpeech/dev-clean/84'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker_84_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of waveform: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample rate of waveform: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchaudio/backend/sox_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# check if valid file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found or is a directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# initialize output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /content/drive/MyDrive/DeepProject/wav2vec/LibriSpeech/dev-clean/84 not found or is a directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KzliWhmM4z"
      },
      "source": [
        "###Get information about audio file include the duration time in seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKQpj0VElfL0",
        "outputId": "60154bb9-0fc9-46bb-de0f-5ba9c3fa684b"
      },
      "source": [
        "print(sf.info(f'{speaker_84_data_dir}/121123/84-121123-0003.flac'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DeepProject/wav2vec/LibriSpeech/dev-clean/84/121123/84-121123-0003.flac\n",
            "samplerate: 16000 Hz\n",
            "channels: 1\n",
            "duration: 6.800 s\n",
            "format: FLAC (Free Lossless Audio Codec) [FLAC]\n",
            "subtype: Signed 16 bit PCM [PCM_16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8j9j6HDwjtq"
      },
      "source": [
        "##On Top Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxdGfXSAwrmv"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRMUgnPQh06k"
      },
      "source": [
        "##Declare Loss functions and some Hyperparametes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyzaaNtbh9sF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mle8wZjCh-KF"
      },
      "source": [
        "##Generic Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oKSikRdiDho"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}