{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wav2vec_SV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danshal/DL_project/blob/develop/wav2vec_SV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wl_onLCsnYA"
      },
      "source": [
        "##Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wZdYezr-O3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7e8c93-d3fb-49d1-9c82-5a8cd101bfaf"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfi2rXAIs9rR"
      },
      "source": [
        "##Install fairseq and more packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMEDrdPctBfp",
        "outputId": "9c5fd22d-2ac4-4e9e-e779-6efd8db70519"
      },
      "source": [
        "!pip install soundfile git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\r\n",
        "!pip install fairseq\r\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\r\n",
        "#!mkdir fairseq_code\r\n",
        "#%cd /content/drive/MyDrive/DeepProject/wav2vec/fairseq_code/\r\n",
        "#!git clone https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\n",
            "  Cloning git://github.com/pytorch/fairseq.git (to revision b8ea8a9b72c82192da07e3377adf4ebbde16716d) to /tmp/pip-req-build-wgn0rnm8\n",
            "  Running command git clone -q git://github.com/pytorch/fairseq.git /tmp/pip-req-build-wgn0rnm8\n",
            "  Running command git checkout -q b8ea8a9b72c82192da07e3377adf4ebbde16716d\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): fairseq==1.0.0a0+b8ea8a9 from git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (0.8)\n",
            "Requirement already satisfied: hydra-core<1.1 in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.0.5)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.5.0)\n",
            "Requirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (2019.12.20)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (2.0.6)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (1.7.1+cu101)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+b8ea8a9) (0.29.21)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (5.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (4.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+b8ea8a9) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+b8ea8a9) (3.7.4.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+b8ea8a9) (5.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==1.0.0a0+b8ea8a9) (2.20)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+b8ea8a9) (3.4.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+b8ea8a9-cp36-cp36m-linux_x86_64.whl size=2731763 sha256=884fb14e405c83fd6e4e3c30162e7429edd7b9f1f185a3703609dbc69eb60955\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aqlwj4e4/wheels/39/26/93/08a9f837c7e94436027d116b1ffe57822725b2caf742649fb3\n",
            "Successfully built fairseq\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (1.0.0a0+b8ea8a9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.1+cu101)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.4)\n",
            "Requirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.19.5)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: hydra-core<1.1 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.0.5)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.2.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq) (5.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1->fairseq) (4.8)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aagqz11s7f1"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk5-AwDdtm9o"
      },
      "source": [
        "#General imports\r\n",
        "import argparse\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import os.path as osp\r\n",
        "import pprint\r\n",
        "import soundfile as sf\r\n",
        "from typing import Tuple\r\n",
        "#facebook team framework\r\n",
        "import fairseq\r\n",
        "#pytorch\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torchaudio\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchaudio.datasets.utils import (\r\n",
        "    download_url,\r\n",
        "    extract_archive,\r\n",
        "    walk_files,\r\n",
        ")\r\n",
        "\r\n",
        "#matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#numpy\r\n",
        "import numpy as np\r\n",
        "#pandas\r\n",
        "import pandas as pd\r\n",
        "try:\r\n",
        "    import tqdm\r\n",
        "except:\r\n",
        "    print(\"Install tqdm to use --log-format=tqdm\")\r\n",
        "\r\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPN7qCOwll0k",
        "outputId": "9c6c965b-9cb4-4270-e73e-42dd29af722c"
      },
      "source": [
        "import torch\r\n",
        "print(torch.__version__)\r\n",
        "!nvcc --version\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)\r\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.1+cu101\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVmPE6q7t_Eq"
      },
      "source": [
        "##Get Trained Model & Try Sanity Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nedhgxmcuDaS",
        "outputId": "10cad435-1377-4aef-92e2-6ac0d3caa7c9"
      },
      "source": [
        "cp_path = cp_path = '/content/drive/My Drive/DeepProject/wav2vec/wav2vec_large.pt'\r\n",
        "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n",
        "model = model[0]\r\n",
        "model.eval()\r\n",
        "\r\n",
        "wav_input_16khz = torch.randn(1,10000)\r\n",
        "z = model.feature_extractor(wav_input_16khz)\r\n",
        "c = model.feature_aggregator(z)\r\n",
        "print(f'input shape = {wav_input_16khz.shape} ; z shape = {z.shape} ; z shape = {c.shape}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape = torch.Size([1, 10000]) ; z shape = torch.Size([1, 512, 60]) ; z shape = torch.Size([1, 512, 60])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euSpEw7Z3QT0"
      },
      "source": [
        "#@title Hyperparametes\n",
        "batch_size = 32\n",
        "epochs = 100\n"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-sCUryI1ptJ"
      },
      "source": [
        "###Download Dataset\r\n",
        "###Information about LibriSpeech dataset:\r\n",
        "1. Total of 982 hours of 16KHz read English speech\r\n",
        "2. 2484 speakers in this corpus.\r\n",
        "3. Suppose to be reasonably balances in terms of gender and per-speaker duration\r\n",
        "4. Allowed urls for LIBRISPEECH - \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\", \"train-clean-100\", \"train-clean-360\" and \"train-other-500\". (default: \"train-clean-100\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6KWwI91ruB"
      },
      "source": [
        "# rootDir = '/content/drive/MyDrive/DeepProject/wav2vec/'\r\n",
        "# # Download training data\r\n",
        "# train_data = torchaudio.datasets.LIBRISPEECH(f'{rootDir}/dataset/', download=True)\r\n",
        "# # Download test data\r\n",
        "# test_data = torchaudio.datasets.LIBRISPEECH(f'{rootDir}/dataset/', url='test-clean', download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRH31dJMM9OG"
      },
      "source": [
        "##Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9sfMLUeM_0_"
      },
      "source": [
        "def read_audio(fname):\r\n",
        "    ''' Load an audio file and return PCM along with the sample rate '''\r\n",
        "\r\n",
        "    wav, sr = sf.read(fname)\r\n",
        "    assert sr == 16e3\r\n",
        "\r\n",
        "    return wav, 16e3\r\n",
        "\r\n",
        "\r\n",
        "def audio2tensor(fname):\r\n",
        "  ''' This function will convert audio samples to tensor '''\r\n",
        "  input, _ = read_audio(fname)\r\n",
        "  input_tensor = torch.from_numpy(input).float()\r\n",
        "  return input_tensor.reshape(1, input_tensor.size(0))\r\n",
        "\r\n",
        "\r\n",
        "def get_audio_repr(fname):\r\n",
        "  ''' This function will get audio as input and will output its representation\r\n",
        "      as wav2vec model outputs '''\r\n",
        "  input_tensor = audio2tensor(fname)\r\n",
        "  z = model.feature_extractor(input_tensor)\r\n",
        "  c = model.feature_aggregator(z)\r\n",
        "  return c\r\n",
        "\r\n",
        "\r\n",
        "def get_tensor_repr(input_tensor):\r\n",
        "  '''This function will get a tensor and return its representation as wav2vec\r\n",
        "     model outputs'''\r\n",
        "  with torch.no_grad():\r\n",
        "    z = model.feature_extractor(input_tensor)\r\n",
        "    c = model.feature_aggregator(z)\r\n",
        "    return c\r\n",
        "\r\n",
        "\r\n",
        "def get_sv_example_generator(data):\r\n",
        "  '''This function gets data that loaded as LIBRISPEECH dataset and yields its\r\n",
        "     wav2vec tensor representation with the speaker id''' \r\n",
        "  train_data_iter = iter(data)\r\n",
        "  for i in train_data_iter:\r\n",
        "    yield [get_tensor_repr(i[0]), i[3]]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRHqwROyKmRy"
      },
      "source": [
        "##Make My Own Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtQG9p38KNyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ff6902-adc0-49d5-8141-765acea669d4"
      },
      "source": [
        "rootDir = '/content/drive/MyDrive/DeepProject/wav2vec/'\r\n",
        "URL = \"train-clean-100\"\r\n",
        "FOLDER_IN_ARCHIVE = \"SV_Librispeech_Dataset\"\r\n",
        "FOLDER_IN_ARCHIVE_ORIGINAL_LIBRI = \"LibriSpeech\"\r\n",
        "_CHECKSUMS = {\r\n",
        "    \"http://www.openslr.org/resources/12/dev-clean.tar.gz\":\r\n",
        "    \"76f87d090650617fca0cac8f88b9416e0ebf80350acb97b343a85fa903728ab3\",\r\n",
        "    \"http://www.openslr.org/resources/12/dev-other.tar.gz\":\r\n",
        "    \"12661c48e8c3fe1de2c1caa4c3e135193bfb1811584f11f569dd12645aa84365\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-clean.tar.gz\":\r\n",
        "    \"39fde525e59672dc6d1551919b1478f724438a95aa55f874b576be21967e6c23\",\r\n",
        "    \"http://www.openslr.org/resources/12/test-other.tar.gz\":\r\n",
        "    \"d09c181bba5cf717b3dee7d4d592af11a3ee3a09e08ae025c5506f6ebe961c29\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\":\r\n",
        "    \"d4ddd1d5a6ab303066f14971d768ee43278a5f2a0aa43dc716b0e64ecbbbf6e2\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\":\r\n",
        "    \"146a56496217e96c14334a160df97fffedd6e0a04e66b9c5af0d40be3c792ecf\",\r\n",
        "    \"http://www.openslr.org/resources/12/train-other-500.tar.gz\":\r\n",
        "    \"ddb22f27f96ec163645d53215559df6aa36515f26e01dd70798188350adcb6d2\"\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "size = (100, 100)\r\n",
        "#filter options for tranform are:\r\n",
        "#1. PIL.Image.NEAREST -> Pick one nearest pixel from the input image. Ignore all other input pixels.\r\n",
        "#2. PIL.Image.BILINEAR -> For resize calculate the output pixel value using linear interpolation on all pixels that may contribute to the output value. For other transformations linear interpolation over a 2x2 environment in the input image is used.\r\n",
        "#3. PIL.Image.HAMMING -> Produces a sharper image than BILINEAR, doesn’t have dislocations on local level like with BOX.\r\n",
        "#4. PIL.Image.BICUBIC -> For resize calculate the output pixel value using cubic interpolation on all pixels that may contribute to the output value. For other transformations cubic interpolation over a 4x4 environment in the input image is used.\r\n",
        "#5. PIL.Image.LANCZOS -> Calculate the output pixel value using a high-quality Lanczos filter (a truncated sinc) on all pixels that may contribute to the output value\r\n",
        "#For more details -> https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\r\n",
        "transform = transforms.Resize(size)\r\n",
        "\r\n",
        "\r\n",
        "def load_sv_librispeech_item(fileid: str, path: str, ext_audio: str,\r\n",
        "                          wav2vec_fine_tuning: bool) -> Tuple[Tensor, int]:\r\n",
        "    '''This function will get path to audio file and will return a list of\r\n",
        "       this audio representation as wav2vec model produces with the speaker_id\r\n",
        "       Updata -> returning wav2vec is too heavy and causes runtime reset!\r\n",
        "       Maybe working with google VM will solve this issue.'''\r\n",
        "    speaker_id, utterance_id = fileid.split(\"-\")\r\n",
        "    #fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\r\n",
        "    fileid_audio = speaker_id + \"-\" + utterance_id\r\n",
        "    file_audio = fileid_audio + ext_audio\r\n",
        "    file_audio = os.path.join(path, speaker_id, file_audio)\r\n",
        "    waveform, _ = torchaudio.load(file_audio) # Load audio - dont care about sample rate\r\n",
        "    if wav2vec_fine_tuning == False:\r\n",
        "      waveform_repr = get_tensor_repr(waveform)\r\n",
        "      return (waveform_repr, int(speaker_id))\r\n",
        "    return (waveform, int(speaker_id))\r\n",
        "    #with torch.no_grad():\r\n",
        "      #transformed_waveform_repr = transform(waveform_repr)\r\n",
        "    #return transformed_waveform_repr, int(speaker_id)\r\n",
        "    \r\n",
        "    #return (waveform, int(speaker_id))\r\n",
        "\r\n",
        "\r\n",
        "# create my own collate function to avoid stacking same size examples in a batch\r\n",
        "def my_collate(batch):\r\n",
        "    data = [item[0] for item in batch]  #get 512 X ? tensor in item[0]\r\n",
        "    data = tuple(data)\r\n",
        "    data = torch.cat(data)\r\n",
        "    data = torch.tensor(data, dtype=torch.float)\r\n",
        "    target = [item[1] for item in batch]        \r\n",
        "    target = torch.LongTensor(target)\r\n",
        "    return [data, target]\r\n",
        "\r\n",
        "\r\n",
        "class SV_LIBRISPEECH(Dataset):\r\n",
        "    \"\"\"Create a Dataset for SV_LibriSpeech.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (str): Path to the directory where the dataset is found or downloaded.\r\n",
        "        url (str, optional): The URL to download the dataset from,\r\n",
        "            or the type of the dataset to dowload.\r\n",
        "            Allowed type values are ``\"dev-clean\"``, ``\"dev-other\"``, ``\"test-clean\"``,\r\n",
        "            ``\"test-other\"``, ``\"train-clean-100\"``, ``\"train-clean-360\"`` and\r\n",
        "            ``\"train-other-500\"``. (default: ``\"train-clean-100\"``)\r\n",
        "        folder_in_archive (str, optional):\r\n",
        "            The top-level directory of the dataset. (default: ``\"LibriSpeech\"``)\r\n",
        "        download (bool, optional):\r\n",
        "            Whether to download the dataset if it is not found at root path. (default: ``False``).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    _ext_audio = \".flac\"\r\n",
        "\r\n",
        "    def __init__(self, root: str, url: str = URL,\r\n",
        "                 folder_in_archive: str = FOLDER_IN_ARCHIVE,\r\n",
        "                 download: bool = False, is_SV: bool = True, wav2vec_fine_tuning: bool = False) -> None:\r\n",
        "\r\n",
        "        if url in [\r\n",
        "            \"dev-clean\",\r\n",
        "            \"dev-other\",\r\n",
        "            \"test-clean\",\r\n",
        "            \"test-other\",\r\n",
        "            \"train-clean-100\",\r\n",
        "            \"train-clean-360\",\r\n",
        "            \"train-other-500\",\r\n",
        "        ]:\r\n",
        "            ext_archive = \".tar.gz\"\r\n",
        "            base_url = \"http://www.openslr.org/resources/12/\"\r\n",
        "\r\n",
        "            url = os.path.join(base_url, url + ext_archive)\r\n",
        "\r\n",
        "        basename = os.path.basename(url)\r\n",
        "        archive = os.path.join(root, basename)\r\n",
        "\r\n",
        "        basename = basename.split(\".\")[0]\r\n",
        "        if is_SV == False:\r\n",
        "          folder_in_archive = os.path.join(folder_in_archive, basename)\r\n",
        "\r\n",
        "        self._path = os.path.join(root, folder_in_archive)\r\n",
        "\r\n",
        "        if download:\r\n",
        "            if not os.path.isdir(self._path):\r\n",
        "                if not os.path.isfile(archive):\r\n",
        "                    checksum = _CHECKSUMS.get(url, None)\r\n",
        "                    download_url(url, root, hash_value=checksum)\r\n",
        "                extract_archive(archive)\r\n",
        "        \r\n",
        "        walker = walk_files(self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True)\r\n",
        "        self._walker = list(walker)\r\n",
        "        self.ft = wav2vec_fine_tuning\r\n",
        "  \r\n",
        "\r\n",
        "    def __getitem__(self, n: int) -> Tuple[Tensor, int]:\r\n",
        "        \"\"\"Load the n-th sample from the dataset.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            n (int): The index of the sample to be loaded\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple: ``(waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)``\r\n",
        "        \"\"\"\r\n",
        "        fileid = self._walker[n]  #get file name without .flac suffix        \r\n",
        "        #librispeech_item, speaker_id = load_sv_librispeech_item(fileid, self._path, self._ext_audio)\r\n",
        "\r\n",
        "        return load_sv_librispeech_item(fileid, self._path, self._ext_audio, self.ft)\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self._walker)\r\n",
        "\r\n",
        "\r\n",
        "my_train_data = SV_LIBRISPEECH('/content/drive/MyDrive/DeepProject/wav2vec/', wav2vec_fine_tuning = False)\r\n",
        "print(len(my_train_data))\r\n",
        "my_train_loader = torch.utils.data.DataLoader(my_train_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7I8R5VKOuYD"
      },
      "source": [
        "##Write list of audio lengths to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZfJGaKbOy0-",
        "outputId": "7510c240-abc4-450d-f4d5-ceaa220c38ff"
      },
      "source": [
        "from tabulate import tabulate\r\n",
        "x = list(range(25))\r\n",
        "l = [0, 24, 563, 683, 670, 620, 636, 661, 722, 785, 975, 1379, 2491, 4074, 6012, 6428, 1763, 49, 1, 2, 0, 0, 0, 0, 1]\r\n",
        "table_data = [x, l]\r\n",
        "print(tabulate(table_data, headers='firstrow', tablefmt='fancy_grid'))\r\n",
        "#write it to a file\r\n",
        "with open('histogram.txt', 'w') as f:\r\n",
        "  f.write(tabulate(table_data, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "╒═════╤═════╤═════╤═════╤═════╤═════╤═════╤═════╤═════╤═════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╕\n",
            "│   0 │   1 │   2 │   3 │   4 │   5 │   6 │   7 │   8 │   9 │   10 │   11 │   12 │   13 │   14 │   15 │   16 │   17 │   18 │   19 │   20 │   21 │   22 │   23 │   24 │\n",
            "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╪══════╡\n",
            "│   0 │  24 │ 563 │ 683 │ 670 │ 620 │ 636 │ 661 │ 722 │ 785 │  975 │ 1379 │ 2491 │ 4074 │ 6012 │ 6428 │ 1763 │   49 │    1 │    2 │    0 │    0 │    0 │    0 │    1 │\n",
            "╘═════╧═════╧═════╧═════╧═════╧═════╧═════╧═════╧═════╧═════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╛\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58OZqHFLQ2w"
      },
      "source": [
        "##Just checking it worked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8WkMdWxKSmh",
        "outputId": "04487bff-4b3b-458e-b80a-c2bd0bc93f35"
      },
      "source": [
        "#print(next(iter(my_train_loader)))\r\n",
        "counter = 0\r\n",
        "for i, data in enumerate(my_train_loader, 0):\r\n",
        "  counter = counter + 1\r\n",
        "  if counter > 3:\r\n",
        "    break\r\n",
        "  inputs, labels = data[0].to(device), data[1].to(device)\r\n",
        "  print(inputs[0].shape)\r\n",
        "  print(inputs[1].shape)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n",
            "torch.Size([512, 298])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCmTGT_Nxckv"
      },
      "source": [
        "##Check size of preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QM5NPCNwhl7",
        "outputId": "904be388-7e48-45f1-af12-42dbe045744f"
      },
      "source": [
        "a, s1 = torchaudio.load('/content/drive/MyDrive/DeepProject/wav2vec/SV_Librispeech_Dataset/1088/5.flac')\r\n",
        "b, s2 = torchaudio.load('/content/drive/MyDrive/DeepProject/wav2vec/SV_Librispeech_Dataset/1088/6.flac')\r\n",
        "print(a.shape, s1)\r\n",
        "print(b.shape, s2)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyDrive  Shareddrives\n",
            "torch.Size([1, 48000]) 16000\n",
            "torch.Size([1, 48000]) 16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4d8zcXnI-PH"
      },
      "source": [
        "##Show Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxXLL63s9o8z",
        "outputId": "b359f2a3-7762-4e77-eaf8-c556568bf116"
      },
      "source": [
        "exmps = next(iter(my_train_data))\r\n",
        "count = 0\r\n",
        "for i in iter(my_train_data):\r\n",
        "  count = count + 1\r\n",
        "  print(i)\r\n",
        "  if count == 2:\r\n",
        "    break\r\n",
        "print(exmps[0].shape)  #audio samples as tensor\r\n",
        "print(exmps[1])  #represents speaker id"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[1.1117e-02, 1.1208e-02, 1.5232e-02,  ..., 1.9779e-02,\n",
            "          3.3761e-02, 3.6744e-02],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 6.2024e-03],\n",
            "         [3.4431e-02, 4.2196e-02, 7.5523e-02,  ..., 0.0000e+00,\n",
            "          7.4715e-03, 7.1994e-03],\n",
            "         ...,\n",
            "         [2.9838e-04, 0.0000e+00, 8.9853e-04,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 1.1920e-03],\n",
            "         [2.7779e-01, 2.4062e-01, 2.4504e-01,  ..., 1.6338e-01,\n",
            "          1.6800e-01, 1.7845e-01],\n",
            "         [5.9156e-04, 2.6030e-04, 0.0000e+00,  ..., 3.3415e-01,\n",
            "          3.8136e-01, 3.5824e-01]]]), 103)\n",
            "(tensor([[[0.0053, 0.0044, 0.0099,  ..., 0.0233, 0.0198, 0.0234],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0044, 0.0050],\n",
            "         ...,\n",
            "         [0.0061, 0.0062, 0.0073,  ..., 0.0294, 0.0441, 0.0276],\n",
            "         [0.0795, 0.0917, 0.0316,  ..., 0.0273, 0.0450, 0.0153],\n",
            "         [0.1715, 0.2008, 0.1697,  ..., 0.2998, 0.3033, 0.3735]]]), 103)\n",
            "torch.Size([1, 512, 298])\n",
            "103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "cl6hfTJl30r0",
        "outputId": "3307defc-1e5f-4829-80b9-94e2b4fe2ffa"
      },
      "source": [
        "waveform, sample_rate = torchaudio.load(f'{speaker_84_data_dir}/121123/84-121123-0003.flac')\r\n",
        "\r\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))\r\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(waveform.t().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of waveform: torch.Size([1, 108800])\n",
            "Sample rate of waveform: 16000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f555f8462b0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KzliWhmM4z"
      },
      "source": [
        "###Get information about audio file include the duration time in seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKQpj0VElfL0",
        "outputId": "60154bb9-0fc9-46bb-de0f-5ba9c3fa684b"
      },
      "source": [
        "print(sf.info(f'{speaker_84_data_dir}/121123/84-121123-0003.flac'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DeepProject/wav2vec/LibriSpeech/dev-clean/84/121123/84-121123-0003.flac\n",
            "samplerate: 16000 Hz\n",
            "channels: 1\n",
            "duration: 6.800 s\n",
            "format: FLAC (Free Lossless Audio Codec) [FLAC]\n",
            "subtype: Signed 16 bit PCM [PCM_16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8j9j6HDwjtq"
      },
      "source": [
        "##On Top Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxdGfXSAwrmv"
      },
      "source": [
        "class NAIVE_SV(nn.Module):\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRMUgnPQh06k"
      },
      "source": [
        "##Declare Loss functions and some Hyperparametes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyzaaNtbh9sF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mle8wZjCh-KF"
      },
      "source": [
        "##Generic Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oKSikRdiDho"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}